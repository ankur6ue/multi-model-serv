import requests
import time
import numpy as np
import pandas as pd
import json
import concurrent.futures
from typing import Tuple, Callable, List, Dict

import matplotlib as mpl
import matplotlib.pyplot as plt

# This script sends a number of POST requests to a ray serve deployment and records the time when each request is sent
# and response received (along with the response status code and content).

# User configurable parameters
# The request payload. "data" is the prompt and num_tokens refers to the number of tokens to be generated by the
# model
english_text = {"data": "I'm a software developer, currently working on", "num_tokens": 50}
# The number of requests to be sent
num_requests = 15
# The test duration
test_duration = 20
deployment_url = "http://127.0.0.1:8000/run"


# This function uses the requests libray to send POST request to the Ray serve deployment. It records the start and end
# time for each request and the status code and result content.
def call_model_rest(timeout_sec: float = 30.0) -> Tuple[int, float, float]:

    start_time = time.time()
    content = None
    try:
        print("sending request..")
        result = requests.post(deployment_url, json=english_text, timeout=timeout_sec)
        end_time = time.time()
        status_code = result.status_code
        content = result.content.decode("utf-8")
    except requests.exceptions.Timeout:
        end_time = start_time + timeout_sec
        status_code = 408  # HTTP/408 Request Timeout

    return status_code, start_time, end_time, content


# This function generate uniformly spaced start time for POST requests. The start times could also follow
# a different distribution such a poisson. However keeping things simple with uniformly spaced start times
def gen_start_times(num_requests, test_duration, seed):
    request_interval = float(test_duration) / num_requests
    trace = []
    rng = np.random.default_rng(seed)
    for i in range(num_requests):
        trace.append(request_interval * i)
    return np.array(trace)


seed = 42
# The actual start time for each request (when it is actually sent), which can differ slightly from requested
# start times
actual_start_times = [None] * num_requests
# The times when the response to each request is received
end_times = [None] * num_requests
# The HTTP response codes for each request
result_codes = [None] * num_requests
# The generated text strings
contents = [None] * num_requests
active_requests = {}

# start time for the test
benchmark_start_time = time.time()
# Equally spaced desired start times for each POST request
desired_start_times = (gen_start_times(num_requests, test_duration, seed) + benchmark_start_time)

request_num = 0
# Thread pool to send current requests and track their status
thread_pool = concurrent.futures.ThreadPoolExecutor(1000)
# The function that uses the requests lib to assemble the payload and send POST requests
model_callback = call_model_rest

# while we haven't sent enough requests, or there are some active requests waiting to be processed
while request_num < num_requests or len(active_requests) > 0:
    # Is it time to send the next request? If not, what's the time interval between when the request
    # should be sent and the current time.. That tells us how long we should wait for
    sec_to_next = (
        1.0 if request_num >= num_requests
        else desired_start_times[request_num] - time.time()
    )
    # If it is time to send the next request, do it on a separate thread and record the future in a list
    if sec_to_next <= 0:
        # Time to send the next request
        future = thread_pool.submit(model_callback)
        active_requests[future] = request_num
        request_num += 1
    else:
        # Block until it's time to send the next request or a previous
        # request is done.
        ready_set, _ = concurrent.futures.wait(
            list(active_requests.keys()),
            timeout=sec_to_next)

        # Record timings from any open requests that have completed.
        for future in ready_set:
            request_id = active_requests.pop(future)
            result_code, start_time, end_time, content = future.result()
            actual_start_times[request_id] = start_time
            end_times[request_id] = end_time
            result_codes[request_id] = result_code
            contents[request_id] = content


benchmark_end_time = time.time()
print("time to run benchmark: {:0.3f}".format(benchmark_end_time-benchmark_start_time))
# Collate results as a DataFrame
result = pd.DataFrame({
    'request_id': range(num_requests),
    'desired_start': desired_start_times,
    'actual_start': actual_start_times,
    'end': end_times,
    'result_code': result_codes,
    'contents': contents
})
# Make all times relative to start of the trace
for key in ("desired_start", "actual_start", "end"):
    result[key] -= benchmark_start_time
result["latency"] = result["end"] - result["actual_start"]
mpl.use('TkAgg')
result["latency"].plot()
plt.show()
print('done')